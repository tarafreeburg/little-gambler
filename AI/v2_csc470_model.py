# -*- coding: utf-8 -*-
"""V2_CSC470_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cCkC4vG4svulJmrdhGgJ2ZYVcKp7x1N2
"""

#just copied this from original file


#Imports hell

import numpy as np
import pandas as pd
from math import sqrt

from sklearn.preprocessing import StandardScaler

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import GridSearchCV #to find good parameters

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

#metric measurement
from sklearn import metrics
#from sklearn.metrics import accuracy_score


#for saving the model
import joblib
import pickle
from sklearn.decomposition import LatentDirichletAllocation

#setting up the encoders, scalers, and whatever else

encoder = OneHotEncoder(sparse_output = False)
scaler = StandardScaler()
label_encoder = LabelEncoder()

pd.set_option('display.max_columns', None) #setting up display so I can see stuff easier

games = pd.read_csv('matches.csv')

#things to drop: Unamed: 0, Comp, Notes, Season, Attendance, Match Report
games = games.drop(['Unnamed: 0', 'Comp', 'Notes', 'Season', 'Attendance', 'Match Report', 'Day'], axis = 1)
#keurbel said we can drop the following
games = games.drop(['Captain', 'Formation', 'Dist'], axis = 1)

#stuff we aren't going to know
games = games.drop(['GF', 'GA', 'Poss', 'Sh', 'SoT', 'FK', 'PK', 'PKatt'], axis = 1)

print(games.columns)
print(games.head(35))

#placing 'Opponent_' before the opponent team name so we don't get models of the same
games['Opponent'] = 'Opponent_' + games['Opponent']

#Encoding ~~hell~~ Fun

Team_encoded = encoder.fit_transform(games[['Team']])
Team_encoded_df = pd.DataFrame(Team_encoded, columns=encoder.categories_[0])
games = pd.concat([games.drop('Team', axis = 1), Team_encoded_df], axis=1)

Referee_encoded = encoder.fit_transform(games[['Referee']])
Referee_encoded_df = pd.DataFrame(Referee_encoded, columns=encoder.categories_[0])
games = pd.concat([games.drop('Referee', axis = 1), Referee_encoded_df], axis=1)

Opponent_encoded = encoder.fit_transform(games[['Opponent']])
Opponent_encoded_df = pd.DataFrame(Opponent_encoded, columns=encoder.categories_[0])
games = pd.concat([games.drop('Opponent', axis = 1), Opponent_encoded_df], axis=1)

#having our y be one-hot encoded is a mess, simple encoding is fineeeeeee
#Result_encoded = encoder.fit_transform(games[['Result']])
#Result_encoded_df = pd.DataFrame(Result_encoded, columns=encoder.categories_[0])
#games = pd.concat([games.drop('Result', axis = 1), Result_encoded_df], axis = 1)


#simple encoding
games['Venue'] = label_encoder.fit_transform(games['Venue'])
games['Result'] = label_encoder.fit_transform(games['Result'])


#removing the word 'matchweek X' to only include the number X

games['Round'] = games['Round'].str.replace('Matchweek ', '', regex=False)

# Convert the result to an integer (optional)
games['Round'] = games['Round'].astype(int)



#making 'date' to unix time
games["DateTime"] = pd.to_datetime(games["Date"] + " " + games["Time"])
games["Unix_Time"] = games["DateTime"].astype('int64') // 10**9

games = games.drop(['Date', 'Time', 'DateTime'], axis = 1)
print(games.columns)
print(games.head(35))

#creating the dataframes
X = games[['Round', 'Venue', 'xG', 'xGA',
      'Arsenal', 'AstonVilla', 'Bournemouth',
       'Brentford', 'BrightonandHoveAlbion', 'Burnley', 'Chelsea',
       'CrystalPalace', 'Everton', 'Fulham', 'Liverpool', 'LutonTown',
       'ManchesterCity', 'ManchesterUnited', 'NewcastleUnited',
       'NottinghamForest', 'SheffieldUnited', 'TottenhamHotspur',
       'WestHamUnited', 'WolverhamptonWanderers', 'Andy Madley',
       'Anthony Taylor', 'Chris Kavanagh', 'Craig Pawson', 'Darren Bond',
       'Darren England', 'David Coote', 'Graham Scott', 'Jarred Gillett',
       'John Brooks', 'Joshua Smith', 'Lewis Smith', 'Matt Donohue',
       'Michael Oliver', 'Michael Salisbury', 'Paul Tierney', 'Peter Bankes',
       'Rebecca Welch', 'Robert Jones', 'Robert Madley', 'Samuel Allison',
       'Samuel Barrott', 'Simon Hooper', 'Stuart Attwell', 'Sunny Singh',
       'Thomas Bramall', 'Tim Robinson', 'Tony Harrington', 'Opponent_Arsenal',
       'Opponent_Aston Villa', 'Opponent_Bournemouth', 'Opponent_Brentford',
       'Opponent_Brighton', 'Opponent_Burnley', 'Opponent_Chelsea',
       'Opponent_Crystal Palace', 'Opponent_Everton', 'Opponent_Fulham',
       'Opponent_Liverpool', 'Opponent_Luton Town', 'Opponent_Manchester City',
       'Opponent_Manchester Utd', 'Opponent_Newcastle Utd',
       'Opponent_Nott\'ham Forest', 'Opponent_Sheffield Utd',
       'Opponent_Tottenham', 'Opponent_West Ham', 'Opponent_Wolves','Unix_Time']]

X = scaler.fit_transform(X)

y = games['Result']

"""BREAKDOWN OF REMAINING FEATURES:

Round: Matchweek of compitition

Venue: Home or Away match

xG: Expected Goals

xGA: Expected Goals on

*Team name*: Team on

*Opponent_Team Name*: Team Facing

*Human name*: Referee

*Unix_Time* Unix Time of when the game started

"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #random state used for testing

#let's fit a basic KNN
base_KNN = KNeighborsClassifier(n_neighbors=3)
base_KNN.fit(X_train, np.ravel(y_train))
basic_knn_y_pred = base_KNN.predict(X_test)

basic_knn_score = base_KNN.score(X_test, y_test)
print('Accuracy score is ', end="")
print('%.3f' % basic_knn_score)

print("")
print("KNN model precision:", metrics.precision_score(np.ravel(y_test), basic_knn_y_pred, average = 'macro'))
print("KNN model recall:", metrics.recall_score(np.ravel(y_test), basic_knn_y_pred, average = 'macro'))
print("KNN model kappa:", metrics.cohen_kappa_score(np.ravel(y_test), basic_knn_y_pred))


basic_knn_confMatrix = metrics.confusion_matrix(np.ravel(y_test), basic_knn_y_pred)
print("\nConfusion matrix:\n", basic_knn_confMatrix)

#basic LDA
basic_LDAmodel = LinearDiscriminantAnalysis()
basic_LDAmodel.fit(X_train, np.ravel(y_train))
basic_LDAModel_pred = basic_LDAmodel.predict(X_test)

basic_LDA_score = basic_LDAmodel.score(X_test, np.ravel(y_test))
print(round(basic_LDA_score, 3))


basic_LDA_confMatrix = metrics.confusion_matrix(np.ravel(y_test), basic_LDAModel_pred)
print("Confusion matrix:\n", basic_LDA_confMatrix)

print("")
print("LDA precision:", metrics.precision_score(np.ravel(y_test), basic_LDAModel_pred, average = 'macro'))
print("LDA recall:", metrics.recall_score(np.ravel(y_test), basic_LDAModel_pred, average = 'macro'))
print("LDA kappa:", metrics.cohen_kappa_score(np.ravel(y_test), basic_LDAModel_pred))

#Basic Logistic Regression
basic_logisticModel = LogisticRegression(penalty = 'l2') #l2 because why not
#NOTE: 'l2' and None both got a score of .979
basic_logisticModel.fit(X_train, np.ravel(y_train))
basic_logisticModel_pred = basic_logisticModel.predict(X_test)

basic_logistic_score = basic_logisticModel.score(X_test, np.ravel(y_test))
print(round(basic_logistic_score, 3))

basic_logisticModel_confMatrix = metrics.confusion_matrix(np.ravel(y_test), basic_logisticModel_pred)
print("Confusion matrix:\n", basic_logisticModel_confMatrix)

print("")
print("Logistic Regression precision:", metrics.precision_score(np.ravel(y_test), basic_logisticModel_pred, average = 'macro'))
print("Logistic Regression recall:", metrics.recall_score(np.ravel(y_test), basic_logisticModel_pred, average = 'macro'))
print("Logistic Regression kappa:", metrics.cohen_kappa_score(np.ravel(y_test), basic_logisticModel_pred))

#basic GaussianNB
basic_GNBModel = GaussianNB()
basic_GNBModel.fit(X_train, np.ravel(y_train))
basic_GNB_pred = basic_GNBModel.predict(X_test)

basic_GNBModel_score = basic_GNBModel.score(X_test, np.ravel(y_test))
print(round(basic_GNBModel_score, 3))

basic_GNB_confMatrix = metrics.confusion_matrix(np.ravel(y_test), basic_GNB_pred)
print("Confusion matrix:\n", basic_GNB_confMatrix)

print("")
print("GNB precision:", metrics.precision_score(np.ravel(y_test), basic_GNB_pred, average = 'macro'))
print("GNB recall:", metrics.recall_score(np.ravel(y_test), basic_GNB_pred, average = 'macro'))
print("GNB kappa:", metrics.cohen_kappa_score(np.ravel(y_test), basic_GNB_pred))

#Saving the fitted LDA and Logistic Regression Model as they performed the best
#also saving the scale to make sure it is the same

joblib.dump(basic_LDAmodel, 'lda_model.joblib')
joblib.dump(basic_logisticModel, 'LR_model.joblib')
joblib.dump(scaler, 'scaler.joblib')